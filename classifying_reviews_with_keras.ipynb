{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classifying Movie Reviews from IMDb\n",
    "\n",
    "The Internet Movie Database (IMDb) is an online database consisting of movie, TV, and video game information.  Our goal is to analyze a dataset of movie reviews from IMDb and use the Python neural network library [Keras](https://keras.io/) to predict the sentiment of a movie review.  The dataset consists of 25,000 movie reviews from IMDb, labeled by sentiment (positive review or negative review). Reviews have been preprocessed, and each review is encoded as a sequence of word indices (integers).  Words are indexed by overall frequency in the dataset, so for example the integer 1 corresponds to the most frequent word in the dataset, 2 corresponds to the 2nd most frequent word, etc.  Therefore, a sentence is represented by a sequence of integers, and each movie review can be encoded as an integer vector.\n",
    "\n",
    "For humans, it is easy to predict the sentiment of a review.  This is something we want to train the model to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and prepare the data\n",
    "\n",
    "With Keras, this dataset comes [preloaded](https://keras.io/datasets/), so a simple command will allow us to access the training and testing data. The parameter `num_words` specifies the top most frequent words to consider.  The training and testing data are loaded into feature matrices `X_train` and `X_test` (represented as numpy arrays), and the corresponding training/testing labels (`y_train/y_test`) are vectors of binary integers (0 or 1), where 0 represents a bad review and 1 represents a good review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples:  25000\n",
      "number of testing examples:  25000\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split it into 50% train, 50% test sets\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "print('number of training examples: ', len(X_train))\n",
    "print('number of testing examples: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can look at the first training example, which is a movie review encoded as an array of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How to recover the orginal review?\n",
    "\n",
    "The above array is how the algorithm will see the review, but it would be nice to see the review in human-readable form as well, so we can get an idea of the review's sentiment.  There is a function in Keras call `get_word_index` that returns a dictionary mapping words to their corresponding indices.  For example, we can have a look at (say, 10) random entries from the dictionary with the restriction that these 10 entries come from the top 5000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under 464\n",
      "sally 3507\n",
      "coherent 3981\n",
      "floor 1870\n",
      "wont 4280\n",
      "roger 2474\n",
      "ken 3659\n",
      "doctor 1039\n",
      "sensitive 2725\n",
      "each 254\n"
     ]
    }
   ],
   "source": [
    "# print out 10 random entries from the top 5000 most frequent words in the word_index dict\n",
    "word_index = imdb.get_word_index()\n",
    "rand_indices = np.random.randint(5000, size=10)\n",
    "for k,v in word_index.items():\n",
    "    if v in rand_indices:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can use this word index and write a function that translates an integer-encoded review back into human-readable form.  For more information, see the first answer to this [stackoverflow post](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review is clearly positive, so it should get a label of 1 :\n",
      "\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all \n",
      "\n",
      "The second review is clearly negative, so it should get a label of 0 :\n",
      "\n",
      "<START> big hair big <UNK> bad music and a giant safety <UNK> these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an <UNK> the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are <UNK> and funny in equal <UNK> the hair is big lots of <UNK> <UNK> men wear those cut <UNK> <UNK> that show off their <UNK> <UNK> that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music <UNK> and <UNK> taking away bodies and the <UNK> still doesn't close for <UNK> all <UNK> aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n"
     ]
    }
   ],
   "source": [
    "def get_review(review_number):\n",
    "    '''\n",
    "    Put the review back in a form that humans can understand\n",
    "    '''\n",
    "    word_index = imdb.get_word_index()\n",
    "    word_index = {k:v+3 for k, v in word_index.items()}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2\n",
    "    index_word = {v:k for k, v in word_index.items()}\n",
    "    return ' '.join( index_word[i] for i in X_train[review_number] )\n",
    "\n",
    "# print out the first two reviews and their labels\n",
    "print('The first review is clearly positive, so it should get a label of {} :\\n'.format(y_train[0]))\n",
    "print(get_review(0), '\\n')\n",
    "print('The second review is clearly negative, so it should get a label of {} :\\n'.format(y_train[1]))\n",
    "print(get_review(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sequence padding\n",
    "\n",
    "The neural network model requires that the input vectors all have the same length, so we need to set a fixed length for the inputs and then find a way to deal with the movie reviews that are too long or too short.  This can be done using [sequence.pad_sequences](https://keras.io/preprocessing/sequence/) in the `keras.preprocessing` module, which will truncate longer reviews to a fixed length and also pad shorter reviews with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test set dimensions:  (25000, 500) (25000, 500) \n",
      "\n",
      "first sentence in training set, encoded and padded:\n",
      "\n",
      "  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    1   14   22\n",
      "   16   43  530  973 1622 1385   65  458 4468   66 3941    4  173   36  256\n",
      "    5   25  100   43  838  112   50  670    2    9   35  480  284    5  150\n",
      "    4  172  112  167    2  336  385   39    4  172 4536 1111   17  546   38\n",
      "   13  447    4  192   50   16    6  147 2025   19   14   22    4 1920 4613\n",
      "  469    4   22   71   87   12   16   43  530   38   76   15   13 1247    4\n",
      "   22   17  515   17   12   16  626   18    2    5   62  386   12    8  316\n",
      "    8  106    5    4 2223    2   16  480   66 3785   33    4  130   12   16\n",
      "   38  619    5   25  124   51   36  135   48   25 1415   33    6   22   12\n",
      "  215   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26  400\n",
      "  317   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194    2   18    4  226   22   21  134  476   26\n",
      "  480    5  144   30    2   18   51   36   28  224   92   25  104    4  226\n",
      "   65   16   38 1334   88   12   16  283    5   16 4472  113  103   32   15\n",
      "   16    2   19  178   32] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "print('train/test set dimensions: ', X_train.shape, X_test.shape, '\\n')\n",
    "print('first sentence in training set, encoded and padded:\\n\\n ', X_train[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model architecture\n",
    "\n",
    "### 1. Using GridSearchCV to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(optimizer='rmsprop', input_dim=5000, output_dim=32, input_length=500):\n",
    "    '''\n",
    "    Setup the model architecture and compile the model.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    model.add(Embedding(input_dim, output_dim, input_length=max_review_length))\n",
    "    \n",
    "    # first hidden layer \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # second hidden layer\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1, activation='logistic'))\n",
    "    \n",
    "    # compile the model\n",
    "    #optimizer = RMSprop(lr=learn_rate, rho=grad_decay)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def grid_search(model, param_grid):\n",
    "    '''\n",
    "    Perform a grid search of model hyperparameters and return the best model,\n",
    "    along with a summary of the different grid combinations.\n",
    "    '''\n",
    "    # set up and perform the grid search\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # the cv_results_ attribute is a dict that summarizes many important results\n",
    "    # from each model\n",
    "    test_score_means = grid_result.cv_results_['mean_test_score']\n",
    "    train_score_means = grid_result.cv_results_['mean_train_score']\n",
    "    train_times = grid_result.cv_results_['mean_fit_time']\n",
    "    params = grid_result.cv_results_['params']\n",
    "\n",
    "    # store these results in a smaller dict for easy loading into a pandas dataframe\n",
    "    final_results = dict(mean_test_score=test_score_means, mean_train_score=train_score_means,\n",
    "                         mean_fit_time=train_times, params=params)\n",
    "\n",
    "    print('Best score {} using hyperparameters {}'.format(grid_result.best_score_, grid_result.best_params_))\n",
    "    \n",
    "    return grid_result, final_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural Network with LSTM Layer\n",
    "\n",
    "Recurrent neural networks and LSTMs are a pretty complicated topic in deep learning, and I'm only at the point where I barely understand these topics myself.  One of the dangers with using a high-level library like Keras (in my opinion) is that it can be way too easy to implement these models without having a clue what the hell you're doing.  In order to really understand Machine Learning (and deep learning in particular), it's important to understand the mathematics behind the algorithms and techniques.  A really nice lecture on recurrent neural networks is available [here](https://www.youtube.com/watch?v=6niqTuYFZLQ&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) (starting at about 9 minutes into the video).  It is from Stanford's [CS 231N Course](http://cs231n.stanford.edu/syllabus.html) on Convolutional Neural Networks for Image Recognition.  There is also a great online deep learning textbook available [here](http://www.deeplearningbook.org/). \n",
    "\n",
    "We will build a sequential model with an embedding layer and an LSTM layer. The sequential model is just a linear stack of layers in the neural network.  The first layer is an [Embedding](https://keras.io/layers/embeddings/) layer that maps each integer-encoded word into a space of word vectors where semantically similar words are mapped to nearby points.  This natural language processing model is called [word2vec](https://www.tensorflow.org/tutorials/word2vec).\n",
    "\n",
    "After the embedding layer, we add an [LSTM](https://keras.io/layers/recurrent/#lstm) layer.  LSTM is an abbreviation for Long Short-Term Memory Unit, and these were originally proposed as a solution to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) (and the exploding gradient problem).  In a nutshell, the vanishing/exploding gradient problem are problems when training neural networks with gradient descent methods and back-propagation.  When a neural network is trained, it tries to minimize a cost function by back-propagating the error through the network and repeatedly computing the gradients of composite functions through the layers.  Depending on the activation functions used, the gradients can either become so small or so large that further weight updates become infeasible.  LSTMs help preserve the error that is back-propagated through the network. \n",
    "\n",
    "We will start by trying the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301.0\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "top_words = 5000\n",
    "embedding_dim = 32\n",
    "max_review_length = 500\n",
    "activation_function = 'sigmoid'\n",
    "loss_function = 'binary_crossentropy'\n",
    "optimization_method = 'adam'\n",
    "evaluation_metrics = ['accuracy']\n",
    "\n",
    "def build_lstm_model(input_dim=top_words, output_dim=embedding_dim, input_length=max_review_length,\n",
    "                activation=activation_function, loss=loss_function,\n",
    "                optimizer=optimization_method, metrics=evaluation_metrics):\n",
    "    '''\n",
    "    Setup the model architecture, compile the model, and give a model summary.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, output_dim, input_length=max_review_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation=activation_function))\n",
    "    model.compile(optimizer, loss, metrics=evaluation_metrics)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "imdb_model = build_model();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# more model parameters\n",
    "\n",
    "def train_model():\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Let's see the accuracy of the model, along with the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: \", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
