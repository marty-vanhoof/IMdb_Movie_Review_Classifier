{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Movie Reviews from IMDb\n",
    "### -Marty VanHoof\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Load and prepare the data](#load)\n",
    "- [How to recover the original review?](#recover)\n",
    "- [Sequence padding](#padding)\n",
    "- [Using GridSearchCV to find optimal hyperparameters](#grid_search)\n",
    " - [1. Tune batch size and number of epochs (and two optimizers)](#batch_epochs)\n",
    " - [2. Tune the training optimization algorithm](#opt_algorithm)\n",
    " - [3. Tune learn rate and learn rate decay for adagrad](#tune_adagrad)\n",
    " - [4. Tune the activation function in the hidden layers](#activation)\n",
    " - [5. Tune dropout regularization](#dropout)\n",
    " - [6. Tune the number of nodes in the hidden layers](#neurons)\n",
    "\n",
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "The Internet Movie Database (IMDb) is an online database consisting of movie, TV, and video game information.  Our goal is to analyze a dataset of movie reviews from IMDb and use the Python neural network library [Keras](https://keras.io/) to predict the sentiment of a movie review.  The dataset consists of 25,000 movie reviews for training (and the same number for testing) from IMDb, labeled by sentiment (positive review or negative review). Reviews have been preprocessed, and each review is encoded as a sequence of word indices (integers).  Words are indexed by overall frequency in the dataset, so for example the integer 1 corresponds to the most frequent word in the dataset, 2 corresponds to the 2nd most frequent word, etc.  Therefore, a sentence is represented by a sequence of integers, and each movie review can be encoded as an integer vector.\n",
    "\n",
    "For humans, it is relatively easy to predict the sentiment of a review.  This is something we want to train the model to do.  Let's start by loading the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.preprocessing import sequence\n",
    "from keras.constraints import max_norm\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    "## Load and prepare the data\n",
    "\n",
    "With Keras, this dataset comes [preloaded](https://keras.io/datasets/), so a simple command will allow us to access the training and testing data. The parameter `num_words` specifies the top most frequent words to consider.  The training and testing data are loaded into matrices `X_train` and `X_test` (represented as numpy arrays), and the corresponding training/testing labels (`y_train/y_test`) are vectors of binary integers (0 or 1), where 0 represents a bad review and 1 represents a good review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples:  25000\n",
      "number of testing examples:  25000\n",
      "classes:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split it into 50% train, 50% test sets\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "print('number of training examples: ', len(X_train))\n",
    "print('number of testing examples: ', len(X_test))\n",
    "print('classes: ', np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the first training example, which is a movie review encoded as an array of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recover'></a>\n",
    "## How to recover the orginal review?\n",
    "\n",
    "The above array is how the algorithm will see the review, but it would be nice to see the review in human-readable form as well, so we can get an idea of the review's sentiment.  There is a function in Keras called `get_word_index()` that returns a dictionary mapping words to their corresponding indices.  For example, we can have a look at (say, 10) random entries from the dictionary with the restriction that these 10 entries come from the top 5000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robert 667\n",
      "until 363\n",
      "trailers 4238\n",
      "bitter 2916\n",
      "carol 3687\n",
      "nyc 4841\n",
      "attracted 3621\n",
      "diamond 3640\n",
      "hero 629\n",
      "psychological 1984\n"
     ]
    }
   ],
   "source": [
    "# print out 10 random entries from the top 5000 most frequent words in the word_index dict\n",
    "word_index = imdb.get_word_index()\n",
    "rand_indices = np.random.randint(5000, size=10)\n",
    "for k,v in word_index.items():\n",
    "    if v in rand_indices:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this word index and write a function that translates an integer-encoded review back into human-readable form.  For more information, see the first answer to this [stackoverflow post](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review is clearly positive, so it should get a label of 1 :\n",
      "\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all \n",
      "\n",
      "The second review is clearly negative, so it should get a label of 0 :\n",
      "\n",
      "<START> big hair big <UNK> bad music and a giant safety <UNK> these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an <UNK> the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are <UNK> and funny in equal <UNK> the hair is big lots of <UNK> <UNK> men wear those cut <UNK> <UNK> that show off their <UNK> <UNK> that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music <UNK> and <UNK> taking away bodies and the <UNK> still doesn't close for <UNK> all <UNK> aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n"
     ]
    }
   ],
   "source": [
    "def get_review(review_number):\n",
    "    '''\n",
    "    Put the review back in a form that humans can understand\n",
    "    '''\n",
    "    word_index = imdb.get_word_index()\n",
    "    word_index = {k:v+3 for k, v in word_index.items()}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2\n",
    "    index_word = {v:k for k, v in word_index.items()}\n",
    "    return ' '.join( index_word[i] for i in X_train[review_number] )\n",
    "\n",
    "# print out the first two reviews and their labels\n",
    "print('The first review is clearly positive, so it should get a label of {} :\\n'.format(y_train[0]))\n",
    "print(get_review(0), '\\n')\n",
    "print('The second review is clearly negative, so it should get a label of {} :\\n'.format(y_train[1]))\n",
    "print(get_review(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='padding'></a>\n",
    "## Sequence padding\n",
    "\n",
    "The neural network model requires that the input vectors all have the same length, so we need to set a fixed length for the inputs and then find a way to deal with the movie reviews that are too long or too short.  This can be done using [sequence.pad_sequences](https://keras.io/preprocessing/sequence/) in the `keras.preprocessing` module, which will truncate longer reviews to a fixed length and also pad shorter reviews with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test set dimensions:  (25000, 500) (25000, 500) \n",
      "\n",
      "first sentence in training set, encoded and padded:\n",
      "\n",
      "  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    1   14   22\n",
      "   16   43  530  973 1622 1385   65  458 4468   66 3941    4  173   36  256\n",
      "    5   25  100   43  838  112   50  670    2    9   35  480  284    5  150\n",
      "    4  172  112  167    2  336  385   39    4  172 4536 1111   17  546   38\n",
      "   13  447    4  192   50   16    6  147 2025   19   14   22    4 1920 4613\n",
      "  469    4   22   71   87   12   16   43  530   38   76   15   13 1247    4\n",
      "   22   17  515   17   12   16  626   18    2    5   62  386   12    8  316\n",
      "    8  106    5    4 2223    2   16  480   66 3785   33    4  130   12   16\n",
      "   38  619    5   25  124   51   36  135   48   25 1415   33    6   22   12\n",
      "  215   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26  400\n",
      "  317   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194    2   18    4  226   22   21  134  476   26\n",
      "  480    5  144   30    2   18   51   36   28  224   92   25  104    4  226\n",
      "   65   16   38 1334   88   12   16  283    5   16 4472  113  103   32   15\n",
      "   16    2   19  178   32] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "print('train/test set dimensions: ', X_train.shape, X_test.shape, '\\n')\n",
    "print('first sentence in training set, encoded and padded:\\n\\n ', X_train[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grid_search'></a>\n",
    "## Using GridSearchCV to find optimal hyperparameters\n",
    "\n",
    "First I want to give a shout-out to Dr. Jason Brownlee, who's [article on grid-searching with Keras](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/) is immensely helpful.\n",
    "\n",
    "[GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) is a very useful class in scikit-learn that performs an exhaustive search over specified hyperparameters of the model and finds those with the most optimal performance.  It saves a lot of time, since now we don't have to tune the hyperparameters manually.  Keras does not have grid search capabilities built into it, but it does have a [scikit-learn wrapper](https://keras.io/scikit-learn-api/) that we can use to interface with the scikit-learn API.  An important thing to note is that we are performing the grid search by tuning a few different hyperparameters separately and then aggregating the results.  This is not the best way to grid search because there may be dependence relationships among the hyperparameters.  There are two reasons I'm doing it this way:  1)  It's too time (and CPU) consuming to tune many hyperparameters at once on my little laptop; 2) tuning the hyperparameters this way the first time is more illustrative and better for my learning process.\n",
    "\n",
    "First we build and compile the model in Keras using the `build_model` function below.  Then we can pass this function as an argument to `KerasClassifier` in order to interface with scikit-learn and use `GridSearchCV`.\n",
    "The `grid_search` function below takes the sklearn-wrapped Keras model and a parameter grid as arguments and then performs the grid search.  It returns a dictionary with some useful summary statistics of the grid combinations and it also prints the accuracy score and the hyperparameters of the optimal model.  We will eventually load the dictionary into a Pandas dataframe to display the results in a nice way.  Most of these steps are implemented in the `main` function in the [imdb_models.py](imdb_models.py) file.  The `main` function will also save the weights from the best model to a `.hdf5` file, and it will write the grid search results to a `.csv` file.  \n",
    "\n",
    "If you are not using some form of cloud computing, then trying to train neural network models in a Jupyter Notebook on your local machine is frustrating, as the process tends to be very slow and will often just crash.  Running the `.py` file directly in the terminal is faster, but in general it's not a good idea to to try train neural network models that are too big on a laptop CPU.  A good option is some form of cloud computing, such as [Amazon Web Services](https://aws.amazon.com/).\n",
    "\n",
    "The model is a sequential model, which is just a linear stack of layers in the neural network.  The first layer is an [Embedding](https://keras.io/layers/embeddings/) layer that maps each integer-encoded word into a space of word vectors where semantically similar words are mapped to nearby points.  This natural language processing model is called [word2vec](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,223,251.0\n",
      "Trainable params: 4,223,251.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(optimizer='adagrad', learn_rate=0.01, learn_rate_decay=0.01, activation='relu',\n",
    "                dropout_rate=0.1, weight_constraint=4, neurons=250, input_dim=5000, output_dim=32,\n",
    "                max_review_length=500):\n",
    "    '''\n",
    "    Setup the model architecture and compile the model.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "\n",
    "    # embedding layer\n",
    "    model.add(Embedding(input_dim, output_dim, input_length=max_review_length))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # first hidden layer \n",
    "    model.add(Dense(neurons, activation=activation, kernel_constraint=max_norm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # second hidden layer\n",
    "    model.add(Dense(neurons, activation=activation, kernel_constraint=max_norm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    optimizer = Adagrad(lr=learn_rate, decay=learn_rate_decay)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def grid_search(model, param_grid):\n",
    "    '''\n",
    "    Performs a grid search of model hyperparameters and returns the best model,\n",
    "    along with some summary statistics of the different grid combinations.\n",
    "    '''\n",
    "    \n",
    "    # set up and perform the grid search\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # the cv_results_ attribute is a dict that summarizes many important results\n",
    "    # from each model\n",
    "    test_score_means = grid_result.cv_results_['mean_test_score']\n",
    "    train_score_means = grid_result.cv_results_['mean_train_score']\n",
    "    train_times = grid_result.cv_results_['mean_fit_time']\n",
    "    params = grid_result.cv_results_['params']\n",
    "\n",
    "    # store these results in a smaller dict for easy loading into a pandas dataframe\n",
    "    final_results = dict(mean_test_score=test_score_means, mean_train_score=train_score_means,\n",
    "                         mean_fit_time=train_times, params=params)\n",
    "\n",
    "    print('Best score {} using hyperparameters {}'.format(grid_result.best_score_,\n",
    "                                                          grid_result.best_params_))\n",
    "    \n",
    "    return grid_result, final_results\n",
    "\n",
    "m = build_model()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='batch_epochs'></a>\n",
    "## 1. Tune batch size and number of epochs (and two optimizers)\n",
    "\n",
    "**Best score using hyperparameters {'optimizer': 'adam', 'batch_size': 200, 'epochs': 2}**\n",
    "\n",
    "We ran the grid search in the [imdb_models.py](imdb_models.py) file with the following parameter grid:\n",
    "\n",
    "```\n",
    "optimizer = ['rmsprop', 'adam']\n",
    "batch_size = [100, 200, 500]\n",
    "epochs = [2, 5, 10]\n",
    "```\n",
    "\n",
    "The function below will display some summary statistics of the different grid combinations, ordered by mean_test_score in descending order.  Only the first 10 results are shown.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 83.01 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.573460</td>\n",
       "      <td>0.86724</td>\n",
       "      <td>0.97924</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 200, 'epochs': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.627119</td>\n",
       "      <td>0.86104</td>\n",
       "      <td>0.98642</td>\n",
       "      <td>{'optimizer': 'rmsprop', 'batch_size': 100, 'epochs': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.803518</td>\n",
       "      <td>0.85900</td>\n",
       "      <td>0.99240</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 100, 'epochs': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.999639</td>\n",
       "      <td>0.85636</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 500, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.485352</td>\n",
       "      <td>0.85552</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 200, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.476564</td>\n",
       "      <td>0.85532</td>\n",
       "      <td>0.95674</td>\n",
       "      <td>{'optimizer': 'rmsprop', 'batch_size': 200, 'epochs': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.653290</td>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 200, 'epochs': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.410074</td>\n",
       "      <td>0.85148</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 100, 'epochs': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.487769</td>\n",
       "      <td>0.85136</td>\n",
       "      <td>0.99996</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 100, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.102054</td>\n",
       "      <td>0.85060</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>{'optimizer': 'rmsprop', 'batch_size': 500, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.573460       0.86724          0.97924            \n",
       "1  1.627119       0.86104          0.98642            \n",
       "2  1.803518       0.85900          0.99240            \n",
       "3  4.999639       0.85636          1.00000            \n",
       "4  7.485352       0.85552          1.00000            \n",
       "5  1.476564       0.85532          0.95674            \n",
       "6  3.653290       0.85204          0.99998            \n",
       "7  4.410074       0.85148          0.99998            \n",
       "8  8.487769       0.85136          0.99996            \n",
       "9  6.102054       0.85060          0.99998            \n",
       "\n",
       "                                                      params  \n",
       "0  {'optimizer': 'adam', 'batch_size': 200, 'epochs': 2}      \n",
       "1  {'optimizer': 'rmsprop', 'batch_size': 100, 'epochs': 2}   \n",
       "2  {'optimizer': 'adam', 'batch_size': 100, 'epochs': 2}      \n",
       "3  {'optimizer': 'adam', 'batch_size': 500, 'epochs': 10}     \n",
       "4  {'optimizer': 'adam', 'batch_size': 200, 'epochs': 10}     \n",
       "5  {'optimizer': 'rmsprop', 'batch_size': 200, 'epochs': 2}   \n",
       "6  {'optimizer': 'adam', 'batch_size': 200, 'epochs': 5}      \n",
       "7  {'optimizer': 'adam', 'batch_size': 100, 'epochs': 5}      \n",
       "8  {'optimizer': 'adam', 'batch_size': 100, 'epochs': 10}     \n",
       "9  {'optimizer': 'rmsprop', 'batch_size': 500, 'epochs': 10}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_results(csv_file, num_rows=0):\n",
    "    '''\n",
    "    Display the model results in a Pandas dataframe\n",
    "    '''\n",
    "    from IPython.display import display\n",
    "    \n",
    "    # load the results from the csv file, sort the values, and reset the index\n",
    "    results = pd.read_csv(csv_file)\n",
    "    results.sort_values(by=['mean_test_score'], inplace=True, ascending=False)\n",
    "    results.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # set the last column to display with the maximum width\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    \n",
    "    # convert mean_fit_time to minutes\n",
    "    pd.to_numeric(results.mean_fit_time)\n",
    "    results['mean_fit_time'] = results.mean_fit_time / 60\n",
    "    \n",
    "    print('\\nTotal training time: {} minutes'.format(round(np.sum(results.mean_fit_time), 2)))\n",
    "    \n",
    "    if num_rows:\n",
    "        display(results.head(num_rows))\n",
    "    else:\n",
    "        display(results)\n",
    "    \n",
    "display_results('imdb_results/grid_batch_epoch_results.csv', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy on the training and test sets\n",
    "\n",
    "We can see that the training accuracy is a lot higher than the accuracy on the test set.  This means that the model has overfit, and this is only after 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 98.02%\n",
      "Test set accuracy: 86.43%\n"
     ]
    }
   ],
   "source": [
    "def check_model(hdf5_file):\n",
    "    '''\n",
    "    Load the weights from the grid search and check the accuracy\n",
    "    on the training and test sets.\n",
    "    '''\n",
    "    from imdb_models import build_model\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    # load the weights that yielded the best validation score from grid search\n",
    "    model.load_weights(hdf5_file)\n",
    "\n",
    "    # evaluate train/test accuracy\n",
    "    train_score = model.evaluate(X_train, y_train, verbose=0)\n",
    "    test_score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    train_accuracy = 100*train_score[1]\n",
    "    test_accuracy = 100*test_score[1]\n",
    "    print('\\nTraining accuracy: {}%'.format(round(train_accuracy, 2)))\n",
    "    print('Test set accuracy: {}%'.format(round(test_accuracy, 2)))\n",
    "    \n",
    "check_model('imdb_results/imdb_batch_epoch_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='opt_algorithm'></a>\n",
    "## 2. Tune the training optimization algorithm\n",
    "\n",
    "**Best score using hyperparameters {'batch_size': 200, 'epochs': 2, 'optimizer': 'adagrad'}**\n",
    "\n",
    "Letting the optimizers vary and choosing the batch size and number of epochs based on the results from 1, we used the following parameter grid:\n",
    "\n",
    "```\n",
    "optimizer = ['adam', 'rmsprop', 'sgd', 'adagrad', 'adadelta', 'adamax', 'nadam']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 9.69 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.322234</td>\n",
       "      <td>0.87756</td>\n",
       "      <td>0.970920</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'adagrad'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.460754</td>\n",
       "      <td>0.86720</td>\n",
       "      <td>0.981940</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'adam'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.278613</td>\n",
       "      <td>0.86424</td>\n",
       "      <td>0.989620</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'nadam'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.346801</td>\n",
       "      <td>0.86328</td>\n",
       "      <td>0.969620</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'rmsprop'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.458367</td>\n",
       "      <td>0.84208</td>\n",
       "      <td>0.922919</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'adamax'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.614987</td>\n",
       "      <td>0.55312</td>\n",
       "      <td>0.581459</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'adadelta'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.207428</td>\n",
       "      <td>0.50816</td>\n",
       "      <td>0.513540</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'sgd'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.322234       0.87756          0.970920           \n",
       "1  1.460754       0.86720          0.981940           \n",
       "2  1.278613       0.86424          0.989620           \n",
       "3  1.346801       0.86328          0.969620           \n",
       "4  1.458367       0.84208          0.922919           \n",
       "5  1.614987       0.55312          0.581459           \n",
       "6  1.207428       0.50816          0.513540           \n",
       "\n",
       "                                                      params  \n",
       "0  {'batch_size': 200, 'epochs': 2, 'optimizer': 'adagrad'}   \n",
       "1  {'batch_size': 200, 'epochs': 2, 'optimizer': 'adam'}      \n",
       "2  {'batch_size': 200, 'epochs': 2, 'optimizer': 'nadam'}     \n",
       "3  {'batch_size': 200, 'epochs': 2, 'optimizer': 'rmsprop'}   \n",
       "4  {'batch_size': 200, 'epochs': 2, 'optimizer': 'adamax'}    \n",
       "5  {'batch_size': 200, 'epochs': 2, 'optimizer': 'adadelta'}  \n",
       "6  {'batch_size': 200, 'epochs': 2, 'optimizer': 'sgd'}       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results('imdb_results/grid_optimizer_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy on the training and test sets\n",
    "\n",
    "There still seems to be overfitting, but the training and test accuracy are a little closer together this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 96.91%\n",
      "Test set accuracy: 87.98%\n"
     ]
    }
   ],
   "source": [
    "check_model('imdb_results/imdb_optimizer_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tune_adagrad'></a>\n",
    "## 3. Tune learn rate and learn rate decay for adagrad\n",
    "\n",
    "**Best score using hyperparameters {'learn_rate_decay': 0.01, 'learn_rate': 0.01}**\n",
    "\n",
    "This time we used adagrad (along with the same setting for batch size and epochs from above), and the following parameter grid:\n",
    "\n",
    "```\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.3]\n",
    "learn_rate_decay = [0.0, 0.1, 0.01, 0.001]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 21.8 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.375478</td>\n",
       "      <td>0.87204</td>\n",
       "      <td>0.943220</td>\n",
       "      <td>{'learn_rate_decay': 0.01, 'learn_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.481537</td>\n",
       "      <td>0.86612</td>\n",
       "      <td>0.958160</td>\n",
       "      <td>{'learn_rate_decay': 0.0, 'learn_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.423921</td>\n",
       "      <td>0.86516</td>\n",
       "      <td>0.949180</td>\n",
       "      <td>{'learn_rate_decay': 0.001, 'learn_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.351528</td>\n",
       "      <td>0.66376</td>\n",
       "      <td>0.725540</td>\n",
       "      <td>{'learn_rate_decay': 0.0, 'learn_rate': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.399060</td>\n",
       "      <td>0.64552</td>\n",
       "      <td>0.704220</td>\n",
       "      <td>{'learn_rate_decay': 0.1, 'learn_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.384608</td>\n",
       "      <td>0.59864</td>\n",
       "      <td>0.656881</td>\n",
       "      <td>{'learn_rate_decay': 0.001, 'learn_rate': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.455008</td>\n",
       "      <td>0.58128</td>\n",
       "      <td>0.637660</td>\n",
       "      <td>{'learn_rate_decay': 0.01, 'learn_rate': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.449480</td>\n",
       "      <td>0.52924</td>\n",
       "      <td>0.555740</td>\n",
       "      <td>{'learn_rate_decay': 0.1, 'learn_rate': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.273191</td>\n",
       "      <td>0.50412</td>\n",
       "      <td>0.497940</td>\n",
       "      <td>{'learn_rate_decay': 0.0, 'learn_rate': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.275629</td>\n",
       "      <td>0.50344</td>\n",
       "      <td>0.498280</td>\n",
       "      <td>{'learn_rate_decay': 0.1, 'learn_rate': 0.3}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.375478       0.87204          0.943220           \n",
       "1  1.481537       0.86612          0.958160           \n",
       "2  1.423921       0.86516          0.949180           \n",
       "3  1.351528       0.66376          0.725540           \n",
       "4  1.399060       0.64552          0.704220           \n",
       "5  1.384608       0.59864          0.656881           \n",
       "6  1.455008       0.58128          0.637660           \n",
       "7  1.449480       0.52924          0.555740           \n",
       "8  1.273191       0.50412          0.497940           \n",
       "9  1.275629       0.50344          0.498280           \n",
       "\n",
       "                                             params  \n",
       "0  {'learn_rate_decay': 0.01, 'learn_rate': 0.01}    \n",
       "1  {'learn_rate_decay': 0.0, 'learn_rate': 0.01}     \n",
       "2  {'learn_rate_decay': 0.001, 'learn_rate': 0.01}   \n",
       "3  {'learn_rate_decay': 0.0, 'learn_rate': 0.001}    \n",
       "4  {'learn_rate_decay': 0.1, 'learn_rate': 0.01}     \n",
       "5  {'learn_rate_decay': 0.001, 'learn_rate': 0.001}  \n",
       "6  {'learn_rate_decay': 0.01, 'learn_rate': 0.001}   \n",
       "7  {'learn_rate_decay': 0.1, 'learn_rate': 0.001}    \n",
       "8  {'learn_rate_decay': 0.0, 'learn_rate': 0.3}      \n",
       "9  {'learn_rate_decay': 0.1, 'learn_rate': 0.3}      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results('imdb_results/grid_learn_rate_results.csv', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy\n",
    "\n",
    "The test set accuracy is a tad higher and the training accuracy has gone down again.  This is a good sign as it means there is less overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 94.75%\n",
      "Test set accuracy: 88.22%\n"
     ]
    }
   ],
   "source": [
    "check_model('imdb_results/imdb_learn_rate_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='activation'></a>\n",
    "## 4. Tune the activation function in the hidden layers\n",
    "\n",
    "**Best score using hyperparameters {'activation': 'relu'}**\n",
    "\n",
    "The activations functions introduce non-linearity into the network and allow the model to learn more complicated functions.  Next we tried the grid search with all the different choices for the activation function in Keras.  Our neural network has 2 hidden layers, and the same activation function is applied in both layers.  The following parameter grid was used:\n",
    "\n",
    "```\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 10.27 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.266569</td>\n",
       "      <td>0.87880</td>\n",
       "      <td>0.951980</td>\n",
       "      <td>{'activation': 'relu'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.281021</td>\n",
       "      <td>0.87812</td>\n",
       "      <td>0.922140</td>\n",
       "      <td>{'activation': 'hard_sigmoid'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.269449</td>\n",
       "      <td>0.86808</td>\n",
       "      <td>0.917900</td>\n",
       "      <td>{'activation': 'sigmoid'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.311112</td>\n",
       "      <td>0.86636</td>\n",
       "      <td>0.899320</td>\n",
       "      <td>{'activation': 'softplus'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.275075</td>\n",
       "      <td>0.86048</td>\n",
       "      <td>0.968440</td>\n",
       "      <td>{'activation': 'softsign'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.348187</td>\n",
       "      <td>0.85704</td>\n",
       "      <td>0.889720</td>\n",
       "      <td>{'activation': 'softmax'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.263466</td>\n",
       "      <td>0.74256</td>\n",
       "      <td>0.814615</td>\n",
       "      <td>{'activation': 'tanh'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.255789</td>\n",
       "      <td>0.73008</td>\n",
       "      <td>0.787662</td>\n",
       "      <td>{'activation': 'linear'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.266569       0.87880          0.951980           \n",
       "1  1.281021       0.87812          0.922140           \n",
       "2  1.269449       0.86808          0.917900           \n",
       "3  1.311112       0.86636          0.899320           \n",
       "4  1.275075       0.86048          0.968440           \n",
       "5  1.348187       0.85704          0.889720           \n",
       "6  1.263466       0.74256          0.814615           \n",
       "7  1.255789       0.73008          0.787662           \n",
       "\n",
       "                           params  \n",
       "0  {'activation': 'relu'}          \n",
       "1  {'activation': 'hard_sigmoid'}  \n",
       "2  {'activation': 'sigmoid'}       \n",
       "3  {'activation': 'softplus'}      \n",
       "4  {'activation': 'softsign'}      \n",
       "5  {'activation': 'softmax'}       \n",
       "6  {'activation': 'tanh'}          \n",
       "7  {'activation': 'linear'}        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results('imdb_results/grid_activation_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy\n",
    "\n",
    "The best performance comes from the relu activation function $g(z) = \\textrm{max}\\{0,z\\}$, which is what we had it set at before, so nothing really changed.  This supports some of the advice given in the [deep learning text](http://www.deeplearningbook.org/contents/mlp.html) (by Goodfellow, Bengio, and Courville) that rectified linear units tend to give better performance and are also easier to optimize because their behavior is closer to linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 94.65%\n",
      "Test set accuracy: 88.28%\n"
     ]
    }
   ],
   "source": [
    "check_model('imdb_results/imdb_activation_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout'></a>\n",
    "## 5. Tune dropout regularization\n",
    "\n",
    "**Best score using hyperparameters {'weight_constraint': 4, 'dropout_rate': 0.1}**\n",
    "\n",
    "As we have seen, overfitting can be a problem with deep neural networks.  Dropout is a regularization technique introduced in a [2014 paper](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) by Srivastava et al.  The idea is that randomly selected nodes are dropped during training, which means that their contribution to the network is temporarily removed on the forward pass, so that any weight updates are not applied to these nodes during backpropagation.\n",
    "\n",
    "We have already been using a dropout value of 0.2 applied to the first and second hidden layers of our network.  The dropout value specifies the probability of a randomly selected node being dropped from the network.  Dropout is also typically applied with a weight constraint that specifies a maximum value for the norms of the weights in each hidden layer, so we also let the weight constraint vary along with the dropout rate:  \n",
    "\n",
    "```\n",
    "dropout_rate = [0.1, 0.2, 0.3, 0.4]\n",
    "weight_constraint = [1, 2, 3, 4] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 26.84 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.513227</td>\n",
       "      <td>0.87720</td>\n",
       "      <td>0.95082</td>\n",
       "      <td>{'weight_constraint': 4, 'dropout_rate': 0.1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.515296</td>\n",
       "      <td>0.87612</td>\n",
       "      <td>0.93998</td>\n",
       "      <td>{'weight_constraint': 3, 'dropout_rate': 0.2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.503684</td>\n",
       "      <td>0.87520</td>\n",
       "      <td>0.95252</td>\n",
       "      <td>{'weight_constraint': 2, 'dropout_rate': 0.1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.526284</td>\n",
       "      <td>0.87324</td>\n",
       "      <td>0.95120</td>\n",
       "      <td>{'weight_constraint': 1, 'dropout_rate': 0.1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.552568</td>\n",
       "      <td>0.87232</td>\n",
       "      <td>0.93172</td>\n",
       "      <td>{'weight_constraint': 3, 'dropout_rate': 0.4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.506878</td>\n",
       "      <td>0.87104</td>\n",
       "      <td>0.95004</td>\n",
       "      <td>{'weight_constraint': 2, 'dropout_rate': 0.2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.531431</td>\n",
       "      <td>0.87048</td>\n",
       "      <td>0.93962</td>\n",
       "      <td>{'weight_constraint': 1, 'dropout_rate': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.538994</td>\n",
       "      <td>0.86808</td>\n",
       "      <td>0.93732</td>\n",
       "      <td>{'weight_constraint': 1, 'dropout_rate': 0.4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.569696</td>\n",
       "      <td>0.86752</td>\n",
       "      <td>0.93222</td>\n",
       "      <td>{'weight_constraint': 4, 'dropout_rate': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.502582</td>\n",
       "      <td>0.86652</td>\n",
       "      <td>0.93718</td>\n",
       "      <td>{'weight_constraint': 3, 'dropout_rate': 0.1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.513227       0.87720          0.95082            \n",
       "1  1.515296       0.87612          0.93998            \n",
       "2  1.503684       0.87520          0.95252            \n",
       "3  1.526284       0.87324          0.95120            \n",
       "4  1.552568       0.87232          0.93172            \n",
       "5  1.506878       0.87104          0.95004            \n",
       "6  1.531431       0.87048          0.93962            \n",
       "7  1.538994       0.86808          0.93732            \n",
       "8  1.569696       0.86752          0.93222            \n",
       "9  1.502582       0.86652          0.93718            \n",
       "\n",
       "                                          params  \n",
       "0  {'weight_constraint': 4, 'dropout_rate': 0.1}  \n",
       "1  {'weight_constraint': 3, 'dropout_rate': 0.2}  \n",
       "2  {'weight_constraint': 2, 'dropout_rate': 0.1}  \n",
       "3  {'weight_constraint': 1, 'dropout_rate': 0.1}  \n",
       "4  {'weight_constraint': 3, 'dropout_rate': 0.4}  \n",
       "5  {'weight_constraint': 2, 'dropout_rate': 0.2}  \n",
       "6  {'weight_constraint': 1, 'dropout_rate': 0.3}  \n",
       "7  {'weight_constraint': 1, 'dropout_rate': 0.4}  \n",
       "8  {'weight_constraint': 4, 'dropout_rate': 0.3}  \n",
       "9  {'weight_constraint': 3, 'dropout_rate': 0.1}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results('imdb_results/grid_dropout_results.csv', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 95.46%\n",
      "Test set accuracy: 88.07%\n"
     ]
    }
   ],
   "source": [
    "check_model('imdb_results/imdb_dropout_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='neurons'></a>\n",
    "## 6.  Tune the number of nodes in the hidden layers (along with batch size and epochs)\n",
    "\n",
    "**Best score {'batch_size': 200, 'neurons': 150, 'epochs': 3}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 46.1 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.430316</td>\n",
       "      <td>0.87864</td>\n",
       "      <td>0.97022</td>\n",
       "      <td>{'batch_size': 200, 'neurons': 150, 'epochs': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.005411</td>\n",
       "      <td>0.87712</td>\n",
       "      <td>0.97524</td>\n",
       "      <td>{'batch_size': 200, 'neurons': 250, 'epochs': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.869987</td>\n",
       "      <td>0.87684</td>\n",
       "      <td>0.97738</td>\n",
       "      <td>{'batch_size': 200, 'neurons': 150, 'epochs': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.080610</td>\n",
       "      <td>0.87652</td>\n",
       "      <td>0.96530</td>\n",
       "      <td>{'batch_size': 100, 'neurons': 250, 'epochs': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.948743</td>\n",
       "      <td>0.87632</td>\n",
       "      <td>0.95834</td>\n",
       "      <td>{'batch_size': 100, 'neurons': 50, 'epochs': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.651717</td>\n",
       "      <td>0.87620</td>\n",
       "      <td>0.96714</td>\n",
       "      <td>{'batch_size': 500, 'neurons': 50, 'epochs': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.879045</td>\n",
       "      <td>0.87592</td>\n",
       "      <td>0.96042</td>\n",
       "      <td>{'batch_size': 100, 'neurons': 150, 'epochs': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.597687</td>\n",
       "      <td>0.87564</td>\n",
       "      <td>0.96538</td>\n",
       "      <td>{'batch_size': 500, 'neurons': 150, 'epochs': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.300412</td>\n",
       "      <td>0.87484</td>\n",
       "      <td>0.95330</td>\n",
       "      <td>{'batch_size': 100, 'neurons': 150, 'epochs': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.322612</td>\n",
       "      <td>0.87452</td>\n",
       "      <td>0.96606</td>\n",
       "      <td>{'batch_size': 200, 'neurons': 250, 'epochs': 3}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.430316       0.87864          0.97022            \n",
       "1  3.005411       0.87712          0.97524            \n",
       "2  1.869987       0.87684          0.97738            \n",
       "3  3.080610       0.87652          0.96530            \n",
       "4  0.948743       0.87632          0.95834            \n",
       "5  0.651717       0.87620          0.96714            \n",
       "6  1.879045       0.87592          0.96042            \n",
       "7  1.597687       0.87564          0.96538            \n",
       "8  1.300412       0.87484          0.95330            \n",
       "9  2.322612       0.87452          0.96606            \n",
       "\n",
       "                                             params  \n",
       "0  {'batch_size': 200, 'neurons': 150, 'epochs': 3}  \n",
       "1  {'batch_size': 200, 'neurons': 250, 'epochs': 4}  \n",
       "2  {'batch_size': 200, 'neurons': 150, 'epochs': 4}  \n",
       "3  {'batch_size': 100, 'neurons': 250, 'epochs': 3}  \n",
       "4  {'batch_size': 100, 'neurons': 50, 'epochs': 4}   \n",
       "5  {'batch_size': 500, 'neurons': 50, 'epochs': 4}   \n",
       "6  {'batch_size': 100, 'neurons': 150, 'epochs': 3}  \n",
       "7  {'batch_size': 500, 'neurons': 150, 'epochs': 4}  \n",
       "8  {'batch_size': 100, 'neurons': 150, 'epochs': 2}  \n",
       "9  {'batch_size': 200, 'neurons': 250, 'epochs': 3}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results('imdb_results/grid_neurons_results.csv', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 95.68%\n",
      "Test set accuracy: 88.38%\n"
     ]
    }
   ],
   "source": [
    "check_model('imdb_results/imdb_neurons_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best accuracy:  88.38%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
