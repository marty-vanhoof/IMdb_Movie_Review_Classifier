{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classifying Movie Reviews from IMDb\n",
    "\n",
    "The Internet Movie Database (IMDb) is an online database consisting of movie, TV, and video game information.  Our goal is to analyze a dataset of movie reviews from IMDb and use the Python neural network library [Keras](https://keras.io/) to predict the sentiment of a movie review.  The dataset consists of 25,000 movie reviews from IMDb, labeled by sentiment (positive review or negative review). Reviews have been preprocessed, and each review is encoded as a sequence of word indices (integers).  Words are indexed by overall frequency in the dataset, so for example the integer 1 corresponds to the most frequent word in the dataset, 2 corresponds to the 2nd most frequent word, etc.  Therefore, a sentence is represented by a sequence of integers, and each movie review can be encoded as an integer vector.\n",
    "\n",
    "For humans, it is easy to predict the sentiment of a review.  This is something we want to train the model to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and prepare the data\n",
    "\n",
    "With Keras, this dataset comes [preloaded](https://keras.io/datasets/), so a simple command will allow us to access the training and testing data. The parameter `num_words` specifies the top most frequent words to consider.  The training and testing data are loaded into feature matrices `X_train` and `X_test` (represented as numpy arrays), and the corresponding training/testing labels (`y_train/y_test`) are vectors of binary integers (0 or 1), where 0 represents a bad review and 1 represents a good review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples:  25000\n",
      "number of testing examples:  25000\n"
     ]
    }
   ],
   "source": [
    "top_words = 5000\n",
    "\n",
    "# Loading the data and split it into 50% train, 50% test sets\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "print('number of training examples: ', len(X_train))\n",
    "print('number of testing examples: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can look at the first training example, which is a movie review encoded as an array of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How to recover the orginal review?\n",
    "\n",
    "The above array is how the algorithm will see the review, but it would be nice to see the review in human-readable form as well, so we can get an idea of the review's sentiment.  There is a function in Keras call `get_word_index` that returns a dictionary mapping words to their corresponding indices.  For example, we can have a look at (say, 10) random entries from the dictionary with the restriction that these 10 entries come from the top 5000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitter 2916\n",
      "psychological 1984\n",
      "trailers 4238\n",
      "until 363\n",
      "robert 667\n",
      "attracted 3621\n",
      "carol 3687\n",
      "diamond 3640\n",
      "nyc 4841\n",
      "hero 629\n"
     ]
    }
   ],
   "source": [
    "# print out 10 random entries from the top 5000 most frequent words in the word_index dict\n",
    "word_index = imdb.get_word_index()\n",
    "rand_indices = np.random.randint(5000, size=10)\n",
    "for k,v in word_index.items():\n",
    "    if v in rand_indices:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can use this word index and write a function that translates an integer-encoded review back into human-readable form.  For more information, see the first answer to this [stackoverflow post](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review is clearly positive, so it should get a label of 1 :\n",
      "\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all \n",
      "\n",
      "The second review is clearly negative, so it should get a label of 0 :\n",
      "\n",
      "<START> big hair big <UNK> bad music and a giant safety <UNK> these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an <UNK> the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are <UNK> and funny in equal <UNK> the hair is big lots of <UNK> <UNK> men wear those cut <UNK> <UNK> that show off their <UNK> <UNK> that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music <UNK> and <UNK> taking away bodies and the <UNK> still doesn't close for <UNK> all <UNK> aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n"
     ]
    }
   ],
   "source": [
    "def get_review(review_number):\n",
    "    '''\n",
    "    Put the review back in a form that humans can understand\n",
    "    '''\n",
    "    word_index = imdb.get_word_index()\n",
    "    word_index = {k:v+3 for k, v in word_index.items()}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2\n",
    "    index_word = {v:k for k, v in word_index.items()}\n",
    "    return ' '.join( index_word[i] for i in X_train[review_number] )\n",
    "\n",
    "# print out the first two reviews and their labels\n",
    "print('The first review is clearly positive, so it should get a label of {} :\\n'.format(y_train[0]))\n",
    "print(get_review(0), '\\n')\n",
    "print('The second review is clearly negative, so it should get a label of {} :\\n'.format(y_train[1]))\n",
    "print(get_review(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sequence padding\n",
    "\n",
    "The neural network model requires that the input vectors all have the same length, so we need to set a fixed length for the inputs and then find a way to deal with the movie reviews that are too long or too short.  This can be done using [sequence.pad_sequences](https://keras.io/preprocessing/sequence/) in the `keras.preprocessing` module, which will truncate longer reviews to a fixed length and also pad shorter reviews with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test set dimensions:  (25000, 500) (25000, 500) \n",
      "\n",
      "first sentence in training set, encoded and padded:\n",
      "\n",
      "  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    1   14   22\n",
      "   16   43  530  973 1622 1385   65  458 4468   66 3941    4  173   36  256\n",
      "    5   25  100   43  838  112   50  670    2    9   35  480  284    5  150\n",
      "    4  172  112  167    2  336  385   39    4  172 4536 1111   17  546   38\n",
      "   13  447    4  192   50   16    6  147 2025   19   14   22    4 1920 4613\n",
      "  469    4   22   71   87   12   16   43  530   38   76   15   13 1247    4\n",
      "   22   17  515   17   12   16  626   18    2    5   62  386   12    8  316\n",
      "    8  106    5    4 2223    2   16  480   66 3785   33    4  130   12   16\n",
      "   38  619    5   25  124   51   36  135   48   25 1415   33    6   22   12\n",
      "  215   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26  400\n",
      "  317   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194    2   18    4  226   22   21  134  476   26\n",
      "  480    5  144   30    2   18   51   36   28  224   92   25  104    4  226\n",
      "   65   16   38 1334   88   12   16  283    5   16 4472  113  103   32   15\n",
      "   16    2   19  178   32] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 500\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print('train/test set dimensions: ', X_train.shape, X_test.shape, '\\n')\n",
    "print('first sentence in training set, encoded and padded:\\n\\n ', X_train[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the  model architecture\n",
    "\n",
    "We will build a sequential model, which is a linear stack of layers in the neural network.  The first layer is an [Embedding](https://keras.io/layers/embeddings/) layer that maps each integer-encoded word into a space of word vectors where semantically similar words are mapped to nearby points.  This natural language processing model is called [word2vec](https://www.tensorflow.org/tutorials/word2vec).\n",
    "\n",
    "After the embedding layer, we add an [LSTM](https://keras.io/layers/recurrent/#lstm) layer.  LSTM is an abbreviation for Long Short-Term Memory Unit, and these were originally proposed as a solution to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).  In a nutshell, the vanishing gradient problem is a problem when training neural networks with gradient descent methods and back-propagation.  When a neural network is trained, it tries to minimize a cost function by back-propagating the error through the network and repeatedly computing the gradients of composite functions through the layers.  Eventually, the gradients become so small that it's very hard to continue updating the weights to train the network.  LSTMs help preserve the error that is back-propagated through the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`Dense` can accept only 1 positional arguments ('units',), but you passed the following positional arguments: [1, 'sigmoid']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0d47a227ca9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-0d47a227ca9e>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_dim, output_dim, input_length, activation, loss, optimizer, metrics)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/martinvanhoof/anaconda/envs/aind-dog/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m                                     \u001b[0;34m', but you passed the following '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                     \u001b[0;34m'positional arguments: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                                     str(list(args[1:])))\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_conversions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: `Dense` can accept only 1 positional arguments ('units',), but you passed the following positional arguments: [1, 'sigmoid']"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "top_words = 5000\n",
    "embedding_dim = 32\n",
    "max_review_length = 500\n",
    "activation_function = 'sigmoid'\n",
    "loss_function = 'binary_crossentropy'\n",
    "optimization_method = 'adam'\n",
    "evaluation_metrics = ['accuracy']\n",
    "\n",
    "def build_model(input_dim=top_words, output_dim=embedding_dim, input_length=max_review_length, \n",
    "                activation=activation_function, loss=loss_function,\n",
    "                optimizer=optimization_method, metrics=evaluation_metrics):\n",
    "    '''\n",
    "    Setup the model architecture, compile the model, and give a model summary.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, output_dim, input_length=max_review_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation))\n",
    "    model.compile(loss, optimizer, metrics)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Let's see the accuracy of the model, along with the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: \", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
