{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classifying Movie Reviews from IMDb\n",
    "### by Marty VanHoof\n",
    "\n",
    "The Internet Movie Database (IMDb) is an online database consisting of movie, TV, and video game information.  Our goal is to analyze a dataset of movie reviews from IMDb and use the Python neural network library [Keras](https://keras.io/) to predict the sentiment of a movie review.  The dataset consists of 25,000 movie reviews from IMDb, labeled by sentiment (positive review or negative review). Reviews have been preprocessed, and each review is encoded as a sequence of word indices (integers).  Words are indexed by overall frequency in the dataset, so for example the integer 1 corresponds to the most frequent word in the dataset, 2 corresponds to the 2nd most frequent word, etc.  Therefore, a sentence is represented by a sequence of integers, and each movie review can be encoded as an integer vector.\n",
    "\n",
    "For humans, it is relatively easy to predict the sentiment of a review.  This is something we want to train the model to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and prepare the data\n",
    "\n",
    "With Keras, this dataset comes [preloaded](https://keras.io/datasets/), so a simple command will allow us to access the training and testing data. The parameter **`num_words`** specifies the top most frequent words to consider.  The training and testing data are loaded into matrices **`X_train`** and **`X_test`** (represented as numpy arrays), and the corresponding training/testing labels (**`y_train/y_test`**) are vectors of binary integers (0 or 1), where 0 represents a bad review and 1 represents a good review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples:  25000\n",
      "number of testing examples:  25000\n",
      "classes:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split it into 50% train, 50% test sets\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "print('number of training examples: ', len(X_train))\n",
    "print('number of testing examples: ', len(X_test))\n",
    "print('classes: ', np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can look at the first training example, which is a movie review encoded as an array of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How to recover the orginal review?\n",
    "\n",
    "The above array is how the algorithm will see the review, but it would be nice to see the review in human-readable form as well, so we can get an idea of the review's sentiment.  There is a function in Keras called **`get_word_index`** that returns a dictionary mapping words to their corresponding indices.  For example, we can have a look at (say, 10) random entries from the dictionary with the restriction that these 10 entries come from the top 5000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wont 4280\n",
      "roger 2474\n",
      "coherent 3981\n",
      "doctor 1039\n",
      "under 464\n",
      "sensitive 2725\n",
      "each 254\n",
      "floor 1870\n",
      "ken 3659\n",
      "sally 3507\n"
     ]
    }
   ],
   "source": [
    "# print out 10 random entries from the top 5000 most frequent words in the word_index dict\n",
    "word_index = imdb.get_word_index()\n",
    "rand_indices = np.random.randint(5000, size=10)\n",
    "for k,v in word_index.items():\n",
    "    if v in rand_indices:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can use this word index and write a function that translates an integer-encoded review back into human-readable form.  For more information, see the first answer to this [stackoverflow post](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review is clearly positive, so it should get a label of 1 :\n",
      "\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all \n",
      "\n",
      "The second review is clearly negative, so it should get a label of 0 :\n",
      "\n",
      "<START> big hair big <UNK> bad music and a giant safety <UNK> these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an <UNK> the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are <UNK> and funny in equal <UNK> the hair is big lots of <UNK> <UNK> men wear those cut <UNK> <UNK> that show off their <UNK> <UNK> that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music <UNK> and <UNK> taking away bodies and the <UNK> still doesn't close for <UNK> all <UNK> aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n"
     ]
    }
   ],
   "source": [
    "def get_review(review_number):\n",
    "    '''\n",
    "    Put the review back in a form that humans can understand\n",
    "    '''\n",
    "    word_index = imdb.get_word_index()\n",
    "    word_index = {k:v+3 for k, v in word_index.items()}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2\n",
    "    index_word = {v:k for k, v in word_index.items()}\n",
    "    return ' '.join( index_word[i] for i in X_train[review_number] )\n",
    "\n",
    "# print out the first two reviews and their labels\n",
    "print('The first review is clearly positive, so it should get a label of {} :\\n'.format(y_train[0]))\n",
    "print(get_review(0), '\\n')\n",
    "print('The second review is clearly negative, so it should get a label of {} :\\n'.format(y_train[1]))\n",
    "print(get_review(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sequence padding\n",
    "\n",
    "The neural network model requires that the input vectors all have the same length, so we need to set a fixed length for the inputs and then find a way to deal with the movie reviews that are too long or too short.  This can be done using [sequence.pad_sequences](https://keras.io/preprocessing/sequence/) in the **`keras.preprocessing`** module, which will truncate longer reviews to a fixed length and also pad shorter reviews with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test set dimensions:  (25000, 500) (25000, 500) \n",
      "\n",
      "first sentence in training set, encoded and padded:\n",
      "\n",
      "  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    1   14   22\n",
      "   16   43  530  973 1622 1385   65  458 4468   66 3941    4  173   36  256\n",
      "    5   25  100   43  838  112   50  670    2    9   35  480  284    5  150\n",
      "    4  172  112  167    2  336  385   39    4  172 4536 1111   17  546   38\n",
      "   13  447    4  192   50   16    6  147 2025   19   14   22    4 1920 4613\n",
      "  469    4   22   71   87   12   16   43  530   38   76   15   13 1247    4\n",
      "   22   17  515   17   12   16  626   18    2    5   62  386   12    8  316\n",
      "    8  106    5    4 2223    2   16  480   66 3785   33    4  130   12   16\n",
      "   38  619    5   25  124   51   36  135   48   25 1415   33    6   22   12\n",
      "  215   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26  400\n",
      "  317   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194    2   18    4  226   22   21  134  476   26\n",
      "  480    5  144   30    2   18   51   36   28  224   92   25  104    4  226\n",
      "   65   16   38 1334   88   12   16  283    5   16 4472  113  103   32   15\n",
      "   16    2   19  178   32] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "print('train/test set dimensions: ', X_train.shape, X_test.shape, '\\n')\n",
    "print('first sentence in training set, encoded and padded:\\n\\n ', X_train[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Using GridSearchCV to find optimal hyperparameters\n",
    "\n",
    "First I want to give a shout-out to Dr. Jason Brownlee, who's [article on grid-searching with Keras](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/) is immensely helpful.\n",
    "\n",
    "[GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) is a very useful class in scikit-learn that performs an exhaustive search over specified hyperparameters of the model and finds those with the most optimal performance.  It saves a lot of time, since now we don't have to tune the hyperparameters manually.  Keras does not have grid search capabilities built into it, but it does have a [scikit-learn wrapper](https://keras.io/scikit-learn-api/) that we can use to interface with the scikit-learn API.  An important thing to note is that we are performing the grid search by tuning a few different hyperparameters separately and then aggregating the results.  This is not the best way to grid search because there may be dependence relationships among the hyperparameters.  There are two reasons I'm doing it this way:  1)  It's too time (and CPU) consuming to tune many hyperparameters at once on my little laptop; 2) tuning the hyperparameters this way the first time is more illustrative and better for my learning process.\n",
    "\n",
    "First we build and compile the model in Keras using the **`build_model`** function below.  Then we can pass this function as an argument to **`KerasClassifier`** in order to interface with scikit-learn and use **`GridSearchCV`**.\n",
    "The **`grid_search`** function below takes the sklearn-wrapped Keras model and a parameter grid as arguments and then performs the grid search.  It returns a dictionary with some useful summary statistics of the grid combinations and it also prints the accuracy score and the hyperparameters of the optimal model.  We will eventually load the dictionary into a Pandas dataframe to display the results in a nice way.  Most of these steps are implemented in the **`main`** function in the [imdb_models.py](https://github.com/marty-vanhoof/IMdb_Movie_Review_Classifier/blob/master/imdb_models.py) file.  The **`main`** function will also save the weights from the best model to a `.hdf5` file, and it will write the grid search results to a `.csv` file.  \n",
    "\n",
    "It's not a good idea to try to train neural network models in a Jupyter Notebook as the process tends to be very slow and will often just crash.  In fact, it's not a great idea to try to train neural networks that are too big on a laptop CPU either.  I'm going to eventually look into some form of cloud computing option, such as [Amazon Web Services](https://aws.amazon.com/).\n",
    "\n",
    "The model is a sequential model, which is just a linear stack of layers in the neural network.  The first layer is an [Embedding](https://keras.io/layers/embeddings/) layer that maps each integer-encoded word into a space of word vectors where semantically similar words are mapped to nearby points.  This natural language processing model is called [word2vec](https://www.tensorflow.org/tutorials/word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,223,251.0\n",
      "Trainable params: 4,223,251.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(optimizer='adagrad', learn_rate=0.01, learn_rate_decay=0.0, activation='relu',\n",
    "                input_dim=5000, output_dim=32, max_review_length=500):\n",
    "    '''\n",
    "    Setup the model architecture and compile the model.\n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # embedding layer\n",
    "    model.add(Embedding(input_dim, output_dim, input_length=max_review_length))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # first hidden layer \n",
    "    model.add(Dense(250, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # second hidden layer\n",
    "    model.add(Dense(250, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    optimizer = Adagrad(lr=learn_rate, decay=learn_rate_decay)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def grid_search(model, param_grid):\n",
    "    '''\n",
    "    Perform a grid search of model hyperparameters and returns the best model,\n",
    "    along with some summary statistics of the different grid combinations.\n",
    "    '''\n",
    "    \n",
    "    # set up and perform the grid search\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # the cv_results_ attribute is a dict that summarizes many important results\n",
    "    # from each model\n",
    "    test_score_means = grid_result.cv_results_['mean_test_score']\n",
    "    train_score_means = grid_result.cv_results_['mean_train_score']\n",
    "    train_times = grid_result.cv_results_['mean_fit_time']\n",
    "    params = grid_result.cv_results_['params']\n",
    "\n",
    "    # store these results in a smaller dict for easy loading into a pandas dataframe\n",
    "    final_results = dict(mean_test_score=test_score_means, mean_train_score=train_score_means,\n",
    "                         mean_fit_time=train_times, params=params)\n",
    "\n",
    "    print('Best score {} using hyperparameters {}'.format(grid_result.best_score_,\n",
    "                                                          grid_result.best_params_))\n",
    "    \n",
    "    return grid_result, final_results\n",
    "\n",
    "m = build_model()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Tune batch size and number of epochs (and two optimizers)\n",
    "\n",
    "**Best score 0.86724 using hyperparameters {'optimizer': 'adam', 'batch_size': 200, 'epochs': 2}**\n",
    "\n",
    "We ran the grid search in the **`imdb_models.py`** file with the following parameter grid:\n",
    "\n",
    "```\n",
    "optimizer = ['rmsprop', 'adam']\n",
    "batch_size = [100, 200, 500]\n",
    "epochs = [2, 5, 10]\n",
    "```\n",
    "\n",
    "The function below will display some summary statistics of the different grid combinations, ordered by mean_test_score in descending order.  Only the first 10 results are shown.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 83.01 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.573460</td>\n",
       "      <td>0.86724</td>\n",
       "      <td>0.97924</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 200, 'epochs': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.627119</td>\n",
       "      <td>0.86104</td>\n",
       "      <td>0.98642</td>\n",
       "      <td>{'optimizer': 'rmsprop', 'batch_size': 100, 'epochs': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.803518</td>\n",
       "      <td>0.85900</td>\n",
       "      <td>0.99240</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 100, 'epochs': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.999639</td>\n",
       "      <td>0.85636</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 500, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.485352</td>\n",
       "      <td>0.85552</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 200, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.476564</td>\n",
       "      <td>0.85532</td>\n",
       "      <td>0.95674</td>\n",
       "      <td>{'optimizer': 'rmsprop', 'batch_size': 200, 'epochs': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.653290</td>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 200, 'epochs': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.410074</td>\n",
       "      <td>0.85148</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 100, 'epochs': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.487769</td>\n",
       "      <td>0.85136</td>\n",
       "      <td>0.99996</td>\n",
       "      <td>{'optimizer': 'adam', 'batch_size': 100, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.102054</td>\n",
       "      <td>0.85060</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>{'optimizer': 'rmsprop', 'batch_size': 500, 'epochs': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.573460       0.86724          0.97924            \n",
       "1  1.627119       0.86104          0.98642            \n",
       "2  1.803518       0.85900          0.99240            \n",
       "3  4.999639       0.85636          1.00000            \n",
       "4  7.485352       0.85552          1.00000            \n",
       "5  1.476564       0.85532          0.95674            \n",
       "6  3.653290       0.85204          0.99998            \n",
       "7  4.410074       0.85148          0.99998            \n",
       "8  8.487769       0.85136          0.99996            \n",
       "9  6.102054       0.85060          0.99998            \n",
       "\n",
       "                                                      params  \n",
       "0  {'optimizer': 'adam', 'batch_size': 200, 'epochs': 2}      \n",
       "1  {'optimizer': 'rmsprop', 'batch_size': 100, 'epochs': 2}   \n",
       "2  {'optimizer': 'adam', 'batch_size': 100, 'epochs': 2}      \n",
       "3  {'optimizer': 'adam', 'batch_size': 500, 'epochs': 10}     \n",
       "4  {'optimizer': 'adam', 'batch_size': 200, 'epochs': 10}     \n",
       "5  {'optimizer': 'rmsprop', 'batch_size': 200, 'epochs': 2}   \n",
       "6  {'optimizer': 'adam', 'batch_size': 200, 'epochs': 5}      \n",
       "7  {'optimizer': 'adam', 'batch_size': 100, 'epochs': 5}      \n",
       "8  {'optimizer': 'adam', 'batch_size': 100, 'epochs': 10}     \n",
       "9  {'optimizer': 'rmsprop', 'batch_size': 500, 'epochs': 10}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_results(csv_file, num_rows=0):\n",
    "    '''\n",
    "    Display the model results in a Pandas dataframe\n",
    "    '''\n",
    "    from IPython.display import display\n",
    "    \n",
    "    # load the results from the csv file, sort the values, and reset the index\n",
    "    results = pd.read_csv(csv_file)\n",
    "    results.sort_values(by=['mean_test_score'], inplace=True, ascending=False)\n",
    "    results.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # set the last column to display with the maximum width\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    \n",
    "    # convert mean_fit_time to minutes\n",
    "    pd.to_numeric(results.mean_fit_time)\n",
    "    results['mean_fit_time'] = results.mean_fit_time / 60\n",
    "    \n",
    "    print('\\nTotal training time: {} minutes'.format(round(np.sum(results.mean_fit_time), 2)))\n",
    "    \n",
    "    if num_rows:\n",
    "        display(results.head(num_rows))\n",
    "    else:\n",
    "        display(results)\n",
    "    \n",
    "display_results('imdb_results/grid_batch_epoch_results.csv', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check accuracy on the training and test sets\n",
    "\n",
    "We can see that the training accuracy is quite a bit higher than the accuracy on the test set.  This means that the model has overfit, and this is only after 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 98.02%\n",
      "Test set accuracy: 86.43%\n"
     ]
    }
   ],
   "source": [
    "def check_model(hdf5_file):\n",
    "    '''\n",
    "    Load the weights from the grid search and check the accuracy\n",
    "    on the test set.\n",
    "    '''\n",
    "    from imdb_models import build_model\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    # load the weights that yielded the best validation score from grid search\n",
    "    model.load_weights(hdf5_file)\n",
    "\n",
    "    # evaluate test accuracy\n",
    "    train_score = model.evaluate(X_train, y_train, verbose=0)\n",
    "    test_score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    train_accuracy = 100*train_score[1]\n",
    "    test_accuracy = 100*test_score[1]\n",
    "    print('\\nTraining accuracy: {}%'.format(round(train_accuracy, 2)))\n",
    "    print('Test set accuracy: {}%'.format(round(test_accuracy, 2)))\n",
    "    \n",
    "check_model('imdb_results/imdb_batch_epoch_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Tune the training optimization algorithm\n",
    "\n",
    "**Best score 0.87756 using hyperparameters {'batch_size': 200, 'epochs': 2, 'optimizer': 'adagrad'}**\n",
    "\n",
    "Letting the optimizers vary and choosing the batch size and number of epochs based on the results from 1, we used the following parameter grid:\n",
    "\n",
    "```\n",
    "optimizer = ['adam', 'rmsprop', 'sgd', 'adagrad', 'adadelta', 'adamax', 'nadam']\n",
    "batch_size = [200]\n",
    "epochs = [2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 9.69 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.322234</td>\n",
       "      <td>0.87756</td>\n",
       "      <td>0.970920</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'adagrad'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.460754</td>\n",
       "      <td>0.86720</td>\n",
       "      <td>0.981940</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'adam'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.278613</td>\n",
       "      <td>0.86424</td>\n",
       "      <td>0.989620</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'nadam'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.346801</td>\n",
       "      <td>0.86328</td>\n",
       "      <td>0.969620</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'rmsprop'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.458367</td>\n",
       "      <td>0.84208</td>\n",
       "      <td>0.922919</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'adamax'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.614987</td>\n",
       "      <td>0.55312</td>\n",
       "      <td>0.581459</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'adadelta'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.207428</td>\n",
       "      <td>0.50816</td>\n",
       "      <td>0.513540</td>\n",
       "      <td>{'batch_size': 200, 'epochs': 2, 'optimizer': 'sgd'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.322234       0.87756          0.970920           \n",
       "1  1.460754       0.86720          0.981940           \n",
       "2  1.278613       0.86424          0.989620           \n",
       "3  1.346801       0.86328          0.969620           \n",
       "4  1.458367       0.84208          0.922919           \n",
       "5  1.614987       0.55312          0.581459           \n",
       "6  1.207428       0.50816          0.513540           \n",
       "\n",
       "                                                      params  \n",
       "0  {'batch_size': 200, 'epochs': 2, 'optimizer': 'adagrad'}   \n",
       "1  {'batch_size': 200, 'epochs': 2, 'optimizer': 'adam'}      \n",
       "2  {'batch_size': 200, 'epochs': 2, 'optimizer': 'nadam'}     \n",
       "3  {'batch_size': 200, 'epochs': 2, 'optimizer': 'rmsprop'}   \n",
       "4  {'batch_size': 200, 'epochs': 2, 'optimizer': 'adamax'}    \n",
       "5  {'batch_size': 200, 'epochs': 2, 'optimizer': 'adadelta'}  \n",
       "6  {'batch_size': 200, 'epochs': 2, 'optimizer': 'sgd'}       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results('imdb_results/grid_optimizer_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check accuracy on the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 96.91%\n",
      "Test set accuracy: 87.98%\n"
     ]
    }
   ],
   "source": [
    "check_model('imdb_results/imdb_optimizer_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tune learn rate and learn rate decay for adagrad\n",
    "\n",
    "**Best score 0.87204 using hyperparameters {'learn_rate_decay': 0.01, 'learn_rate': 0.01}**\n",
    "\n",
    "This time we used adagrad (along with the same setting for batch size and epochs from above), and the following parameter grid:\n",
    "\n",
    "```\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.3]\n",
    "learn_rate_decay = [0.0, 0.1, 0.01, 0.001]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 21.8 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.375478</td>\n",
       "      <td>0.87204</td>\n",
       "      <td>0.943220</td>\n",
       "      <td>{'learn_rate_decay': 0.01, 'learn_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.481537</td>\n",
       "      <td>0.86612</td>\n",
       "      <td>0.958160</td>\n",
       "      <td>{'learn_rate_decay': 0.0, 'learn_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.423921</td>\n",
       "      <td>0.86516</td>\n",
       "      <td>0.949180</td>\n",
       "      <td>{'learn_rate_decay': 0.001, 'learn_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.351528</td>\n",
       "      <td>0.66376</td>\n",
       "      <td>0.725540</td>\n",
       "      <td>{'learn_rate_decay': 0.0, 'learn_rate': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.399060</td>\n",
       "      <td>0.64552</td>\n",
       "      <td>0.704220</td>\n",
       "      <td>{'learn_rate_decay': 0.1, 'learn_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.384608</td>\n",
       "      <td>0.59864</td>\n",
       "      <td>0.656881</td>\n",
       "      <td>{'learn_rate_decay': 0.001, 'learn_rate': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.455008</td>\n",
       "      <td>0.58128</td>\n",
       "      <td>0.637660</td>\n",
       "      <td>{'learn_rate_decay': 0.01, 'learn_rate': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.449480</td>\n",
       "      <td>0.52924</td>\n",
       "      <td>0.555740</td>\n",
       "      <td>{'learn_rate_decay': 0.1, 'learn_rate': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.273191</td>\n",
       "      <td>0.50412</td>\n",
       "      <td>0.497940</td>\n",
       "      <td>{'learn_rate_decay': 0.0, 'learn_rate': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.275629</td>\n",
       "      <td>0.50344</td>\n",
       "      <td>0.498280</td>\n",
       "      <td>{'learn_rate_decay': 0.1, 'learn_rate': 0.3}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.375478       0.87204          0.943220           \n",
       "1  1.481537       0.86612          0.958160           \n",
       "2  1.423921       0.86516          0.949180           \n",
       "3  1.351528       0.66376          0.725540           \n",
       "4  1.399060       0.64552          0.704220           \n",
       "5  1.384608       0.59864          0.656881           \n",
       "6  1.455008       0.58128          0.637660           \n",
       "7  1.449480       0.52924          0.555740           \n",
       "8  1.273191       0.50412          0.497940           \n",
       "9  1.275629       0.50344          0.498280           \n",
       "\n",
       "                                             params  \n",
       "0  {'learn_rate_decay': 0.01, 'learn_rate': 0.01}    \n",
       "1  {'learn_rate_decay': 0.0, 'learn_rate': 0.01}     \n",
       "2  {'learn_rate_decay': 0.001, 'learn_rate': 0.01}   \n",
       "3  {'learn_rate_decay': 0.0, 'learn_rate': 0.001}    \n",
       "4  {'learn_rate_decay': 0.1, 'learn_rate': 0.01}     \n",
       "5  {'learn_rate_decay': 0.001, 'learn_rate': 0.001}  \n",
       "6  {'learn_rate_decay': 0.01, 'learn_rate': 0.001}   \n",
       "7  {'learn_rate_decay': 0.1, 'learn_rate': 0.001}    \n",
       "8  {'learn_rate_decay': 0.0, 'learn_rate': 0.3}      \n",
       "9  {'learn_rate_decay': 0.1, 'learn_rate': 0.3}      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results('imdb_results/grid_learn_rate_results.csv', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy\n",
    "\n",
    "The accuracy is a tad higher and there is less overfitting as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 94.75%\n",
      "Test set accuracy: 88.22%\n"
     ]
    }
   ],
   "source": [
    "check_model('imdb_results/imdb_learn_rate_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tune the activation function in the hidden layers\n",
    "\n",
    "**Best score 0.87804 using hyperparameters {'activation': 'hard_sigmoid'}**\n",
    "\n",
    "The activations functions introduce non-linearity into the network and allow the model to learn more complicated functions.  We will try the grid search with all the different choices for the activation function in Keras.  Our neural network has 2 hidden layers, and the same activation function will be applied in both layers:\n",
    "\n",
    "```\n",
    "['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 10.85 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.375843</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.921720</td>\n",
       "      <td>{'activation': 'hard_sigmoid'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.410847</td>\n",
       "      <td>0.87656</td>\n",
       "      <td>0.953460</td>\n",
       "      <td>{'activation': 'relu'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.377378</td>\n",
       "      <td>0.86892</td>\n",
       "      <td>0.919180</td>\n",
       "      <td>{'activation': 'sigmoid'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.340045</td>\n",
       "      <td>0.86780</td>\n",
       "      <td>0.899880</td>\n",
       "      <td>{'activation': 'softplus'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.383792</td>\n",
       "      <td>0.86312</td>\n",
       "      <td>0.968280</td>\n",
       "      <td>{'activation': 'softsign'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.319819</td>\n",
       "      <td>0.85608</td>\n",
       "      <td>0.890120</td>\n",
       "      <td>{'activation': 'softmax'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.342223</td>\n",
       "      <td>0.75376</td>\n",
       "      <td>0.826056</td>\n",
       "      <td>{'activation': 'tanh'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.296360</td>\n",
       "      <td>0.72960</td>\n",
       "      <td>0.789442</td>\n",
       "      <td>{'activation': 'linear'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_test_score  mean_train_score  \\\n",
       "0  1.375843       0.87804          0.921720           \n",
       "1  1.410847       0.87656          0.953460           \n",
       "2  1.377378       0.86892          0.919180           \n",
       "3  1.340045       0.86780          0.899880           \n",
       "4  1.383792       0.86312          0.968280           \n",
       "5  1.319819       0.85608          0.890120           \n",
       "6  1.342223       0.75376          0.826056           \n",
       "7  1.296360       0.72960          0.789442           \n",
       "\n",
       "                           params  \n",
       "0  {'activation': 'hard_sigmoid'}  \n",
       "1  {'activation': 'relu'}          \n",
       "2  {'activation': 'sigmoid'}       \n",
       "3  {'activation': 'softplus'}      \n",
       "4  {'activation': 'softsign'}      \n",
       "5  {'activation': 'softmax'}       \n",
       "6  {'activation': 'tanh'}          \n",
       "7  {'activation': 'linear'}        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results('imdb_results/grid_activation_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy\n",
    "\n",
    "It seems that this model did quite a bit worse.  I'm a bit perplexed why the results below are so different from the results using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy: 82.82%\n",
      "Test set accuracy: 80.34%\n"
     ]
    }
   ],
   "source": [
    "check_model('imdb_results/imdb_activation_best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural Network with LSTM Layer\n",
    "\n",
    "Recurrent neural networks and LSTMs are a pretty complicated topic in deep learning, and I'm only at the point where I barely understand these topics myself.  One of the dangers with using a high-level library like Keras (in my opinion) is that it can be way too easy to implement these models without having a clue what the hell you're doing.  In order to really understand Machine Learning (and deep learning in particular), it's important to understand the mathematics behind the algorithms and techniques.  A really nice lecture on recurrent neural networks is available [here](https://www.youtube.com/watch?v=6niqTuYFZLQ&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) (starting at about 9 minutes into the video).  It is from Stanford's [CS 231N Course](http://cs231n.stanford.edu/syllabus.html) on Convolutional Neural Networks for Image Recognition.  There is also a great online deep learning textbook available [here](http://www.deeplearningbook.org/). \n",
    "\n",
    "We will build a sequential model with an embedding layer and an LSTM layer. The sequential model is just a linear stack of layers in the neural network.  The first layer is an [Embedding](https://keras.io/layers/embeddings/) layer that maps each integer-encoded word into a space of word vectors where semantically similar words are mapped to nearby points.  This natural language processing model is called [word2vec](https://www.tensorflow.org/tutorials/word2vec).\n",
    "\n",
    "After the embedding layer, we add an [LSTM](https://keras.io/layers/recurrent/#lstm) layer.  LSTM is an abbreviation for Long Short-Term Memory Unit, and these were originally proposed as a solution to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) (and the exploding gradient problem).  In a nutshell, the vanishing/exploding gradient problem are problems when training neural networks with gradient descent methods and back-propagation.  When a neural network is trained, it tries to minimize a cost function by back-propagating the error through the network and repeatedly computing the gradients of composite functions through the layers.  Depending on the activation functions used, the gradients can either become so small or so large that further weight updates become infeasible.  LSTMs help preserve the error that is back-propagated through the network. \n",
    "\n",
    "We will start by trying the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301.0\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "top_words = 5000\n",
    "embedding_dim = 32\n",
    "max_review_length = 500\n",
    "activation_function = 'sigmoid'\n",
    "loss_function = 'binary_crossentropy'\n",
    "optimization_method = 'adam'\n",
    "evaluation_metrics = ['accuracy']\n",
    "\n",
    "def build_lstm_model(input_dim=top_words, output_dim=embedding_dim, input_length=max_review_length,\n",
    "                activation=activation_function, loss=loss_function,\n",
    "                optimizer=optimization_method, metrics=evaluation_metrics):\n",
    "    '''\n",
    "    Setup the model architecture, compile the model, and give a model summary.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, output_dim, input_length=max_review_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation=activation_function))\n",
    "    model.compile(optimizer, loss, metrics=evaluation_metrics)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "imdb_model = build_model();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
